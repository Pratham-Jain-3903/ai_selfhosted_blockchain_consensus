
1
2019 IEEE International Conference on Blockchain (Blockchain)
Decentralized & Collaborative AI on Blockchain
Justin D. Harris
Microsoft Research
Montreal, Canada
justin.harris@microsoft.com
Bo Waggoner
Microsoft Research
New York, USA
bwag@colorado.edu
Abstract—Machine learning has recently enabled large ad-
vances in artificial intelligence, but these tend to be highly cen-
tralized. The large datasets required are generally proprietary;
predictions are often sold on a per-query basis; and published
models can quickly become out of date without effort to acquire
more data and re-train them. We propose a framework for
participants to collaboratively build a dataset and use smart
contracts to host a continuously updated model. This model
will be shared publicly on a blockchain where it can be free
to use for inference. Ideal learning problems include scenarios
where a model is used many times for similar input such as
personal assistants, playing games, recommender systems, etc.
In order to maintain the model’s accuracy with respect to some
test set we propose both financial and non-financial (gamified)
incentive structures for providing good data. A free and open
source implementation for the Ethereum blockchain is provided
at https://github.com/microsoft/0xDeCA10B.
Index Terms—Decentralized AI, Blockchain, Ethereum,
Crowdsourcing, Prediction Markets, Incremental Learning
I. INTRODUCTION
We propose a framework for sharing and improving a
machine learning model. In this framework, anyone can freely
access the model’s predictions or provide data to help im-
prove the model. An important challenge is that the system
must be robust and incentivize participation, but discourage
manipulation. Our framework is modular, and we propose and
justify three example choices of “incentive mechanisms” with
different advantages.
There exist several proposals to combine machine learning
and blockchain frameworks. In systems such as DInEMMo [1],
access to the trained model is limited to a marketplace. This
allows contributors to profit based on a model’s usage, but it
limits access to those who can pay. DanKu proposes storing
already trained models in smart contracts for competitions,
which does not allow for continual updating and collaborative
training [2]. In contrast, the goal of this work is to address
the current centralization of artificial intelligence by sharing
models freely. Such centralization includes machine learning
expertise, siloed proprietary data, and access to machine
learning model predictions (e.g. charged on a per-query basis).
A. Overview
By leveraging advances in AI, prediction markets, and
blockchain platforms, we can demonstrate the capabilities
of a new framework to collect vast amounts of data, allow
contributors to potentially profit, and host a shared machine
learning model as a public resource. The model can be
collaboratively trained by many contributors yet remain open
and free for others to use the model for inference. This is
accomplished with several configurable components:
• the incentive mechanism
• the data handler
• the machine learning model
A smart contract is created and initialized with choices for
these components. It then accepts “add data” actions from
participants, with the incentive mechanism possibly triggering
payments or allowing other actions. Adding data involves
validation from the incentive mechanism, storing in the data
handler, and finally calling the update method on the model’s
contract, as shown in Fig. 1. Prediction is done off-chain by
calling the predict function provided for convenience in the
model’s smart contract code.
The goal of our system is not for the creators to profit:
the goal is to create valuable shared resources. It is possible
for the data contributors to profit financially (depending on the
incentive mechanism) but this is mainly a result of mechanisms
designed to penalize the contributors who submit bad data.
The dataset is also public because it can be found in the
blockchain’s transaction history or through emitted events
(if this feature is available to the blockchain framework).
Collecting large datasets can be costly using typical crowd-
sourcing platforms such as Figure Eight (formerly known as
Dolores Lab, CrowdFlower) and Amazon Mechanical Turk.
In crowdsourcing, filtering out “bad data” is a constant battle
with spammers, who can submit low-effort or nonsensical
data and still receive compensation for their work [3]. In
our incentive mechanisms, contributors do not benefit from
submitting bad data and can even pay a penalty; meanwhile,978-1-7281-4693-5/19/$31.00 c©2019 IEEE DOI 10.1109/Blockchain.2019.00057
arXiv:1907.07247v1  [cs.CR]  16 Jul 2019
honest contributors are actively incentivized to correct others’
mistakes.
B. Machine Learning and Blockchain Background
We mainly considered supervised learning problems where
a dataset consists of labeled samples. For example, in a
recommender system, a movie or restaurant is given a label
from 1 to 5 stars. The term model refers to a machine learning
algorithm that has been trained on data. It is used to make
predictions, e.g. predict the label of a given example. It can
be represented as a neural network, a matrix of numbers, a
decision tree, etc.
Our framework applies to platforms where decentralized
networks agree on a shared sequence of computations. An
example is the Ethereum blockchain [4]. A smart contract is
an object (in the sense of object-oriented programming) in
this shared code. It contains data fields and interacts with new
code and events via its method calls. A computation on-chain
means the computation is done inside of a smart contract.
The input and result of the computation are usually stored on
the blockchain. In contrast, off-chain means the computation
can be done locally on the client’s machine and does not
necessarily need to be public.
In Ethereum, reading and running code provided by a smart
contract has no cost if it does not write to the blockchain.
This means that one can use the model in a smart contract
for inference for free. When we discuss blockchains, smart
contracts, and examples throughout this paper, we are mainly
referring to Ethereum blockchain and the specifics of smart
contracts on the Ethereum platform. However, this design is
certainly not limited to only run on Ethereum.
C. Staking a Deposit
In conventional legal systems, violating an agreement may
result in a penalty or fine. Enforcing a penalty via a smart1
2
(x, y)
      IncentiveMechanism
      DataHandler
addData(x, y)
      Model      Model
3
predict(x) update(x, y)
Fig. 1. Adding data consists of 3 steps. (1) The IncentiveMechanism validates
the transaction, for instance, in some cases a “stake” or monetary deposit is
required. (2) The DataHandler stores data and meta-data onto the blockchain.
This ensures that it is accessible for all future uses, not limited to this smart
contract. (3) The machine learning model is updated according to predefined
training algorithms. In addition to adding data, anyone can query the model
for predictions, and the incentive mechanism may be triggered to provide
users with monetary payments or virtual “karma” points.
contract is complicated because a user cannot be forced to
make a payment. Instead, many solutions in the blockchain
space require users to “stake” deposits that can be re-claimed
later if they obey rules.1 Similarly to those systems, we will
also propose staking a deposit to simplify some incentive
mechanisms for submitting new data.
D. Organization
In section II, we describe the machine learning data handler
and model portion of the algorithm. In section III, we give our
incentive mechanisms. Then we discuss some implementation
details in section IV. In section V, we present some reasons
for using a blockchain for this framework. We present our
responses to several potential issues in section VI. Finally, we
motivate some future work in section VII.
II. MACHINE LEARNING MODELS
This system can be used with various types of models
including supervised and unsupervised. The model architecture
(e.g. different types of neural networks, CRFs [7], SVMs [8],
etc.) chosen relates closely to the incentive mechanism chosen.
In our examples, we mainly consider training supervised
classifiers because they can be used for many applications. We
first propose to leverage the work in the Incremental Learning
space [9] by using models capable of efficiently updating with
one sample. This should lower the transaction costs (“gas”) to
update a model hosted in an Ethereum smart contract because
each data contribution will be small. One simple model with
“cheap” updates is a Nearest Centroid Classifier, also known
as a Rocchio Classifier [10]. Online learning algorithms could
also be appropriate.
A. Initial Model
It is often helpful if the initially deployed model has
been trained to some extent. For example, if the model is
already somewhat useful, it is more likely to receive users
for inference, who are more likely to generate and provide
back data to improve the model. Also, for some but not all of
our incentive mechanisms, having an initial level of accuracy
allows better validation of contributed data. We stress that
although this can be helpful, in many cases the model does
not need to be pre-trained or highly accurate.
B. Limitations
Due to limitations such as the cost of memory in the
Ethereum blockchain, our examples usually focus on applica-
tions related to handling small input that can be compressed
easily, such as text. Text can easily be compressed using
vocabulary dictionaries or with common shared encoders such
as the Universal Sentence Encoder in [11]. Complex models
such as deep neural networks capable of processing images
may be costly to store and update using Ethereum. Just
uploading the raw images would be costly. For example,
1 E.g. many solutions using Proof-of-Stake (PoS) such as in Tendermint
[5] and the Casper system for Ethereum [6] involve staking a deposit to be
considered eligible to participate.
